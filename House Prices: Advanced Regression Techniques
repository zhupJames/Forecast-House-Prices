# Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity
# to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number
# of bedrooms or a white-picket fence.

# With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to
# predict the final price of each home.

# The potential for creative feature engineering provides a rich opportunity for fun and learning. This dataset lends itself to advanced
# regression techniques like random forests and gradient boosting with the popular XGBoost library. We encourage Kagglers to create
# benchmark code and tutorials on Kernels for community learning. Top kernels will be awarded swag prizes at the competition close. 

# I use R to do this competition. I also do a Python version for practice.

library(ggplot2)
library(readr)
library(MASS)
library(plyr)
library(dplyr)
library(lars)
library(moments)
library(caret)
library(gbm)
library(randomForest)
library(rpart)

# input the data. There are two csv documents. train.csv is for data trainning. test.csv is for data testing.
setwd("C:/Users/Pengwei Zhu/Downloads/House Prices Kaggle")
train_org <- read.csv("train.csv", header = TRUE)
test_org <- read.csv("test.csv", header = TRUE)

# check the type of the two data frame
str(train_org)
str(test_org)

# We can see that train has one more variable than test. So I compare all the variables in train and test, and I find that SalePrice is
# missing in test. Therefore, I need to separate train data into two parts, one for train and another for test which is for testing the
# forecasting accuracy of the model generated from train. However, the first thing is to clean the data.

# Here is the function of RMSE which we use this to check how good the model does.
RMSE <- function(x,y){
  a <- sqrt(sum((log(x)-log(y))^2)/length(y))
  return(a)
}

# check missing value
Missing_indices <- sapply(train_org, function(x)sum(is.na(x)))
Missing_Summary <- data.frame(index = names(train_org), Missing_Values = Missing_indices)
Missing_Summary[Missing_Summary$Missing_Values > 0,]

# we can see there are a great number of NAs in some variables, and I find that some of NAs do not mean they are unknown but they mean
# none. Therefore, we'd better change the values of these NAs to "none"

# Alley
levels(train_org$Alley) <- c(levels(train_org$Alley), "None")
train_org$Alley[is.na(train_org$Alley)] <- "None"

# MiscFeature
levels(train_org$MiscFeature) <- c(levels(train_org$MiscFeature), "None")
train_org$MiscFeature[is.na(train_org$MiscFeature)] <- "None"

# Fence
levels(train_org$Fence) <- c(levels(train_org$Fence), "None")
train_org$Fence[is.na(train_org$Fence)] <- "None"

# Pool
levels(train_org$PoolQC) <- c(levels(train_org$PoolQC), "None")
train_org$PoolQC[is.na(train_org$PoolQC)] <- "None"

# Fireplace Qu
levels(train_org$FireplaceQu) <- c(levels(train_org$FireplaceQu), "None")
train_org$FireplaceQu[is.na(train_org$FireplaceQu)] <- "None"

# Garage, I saw GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond which have the same number of NAs. So I think they may
# relate to each other and they may all shows that there is no Garage.
# Therefore, next I should do is to check whether the NAs with the same ID in different variables.
# Caution, some variables like "GarageYrBlt" are integer which is not character, so we change NAs to "0". And the variable "GarageYrBlt"
# shows the year when the garage was built. Since NAs in this varialbe means no garage, we make it to 0 since the year is bigger means it
# is newer and has more value, and no garage should have less value than has garage, so we set the value to 0 is a good choice.
title <- c("GarageType", "GarageYrBlt", "GarageFinish", "GarageQual", "GarageCond")
result <- c()
for (i in 1:(length(title)-1)) {
  if (identical(as.character(train_org[is.na(train_org[,title[i]]) ,title[i]]), as.character(train_org[is.na(train_org[,title[i+1]]),
                                                                                                    title[i+1]]))) {
    result[i] <- paste(title[i], "equal", title[i+1], sep = " ")
  }else {
    result[i] <- paste(title[i], "not equal", title[i+1], sep = " ")
  }
}
print(result)

# Since they all shows the same ID, we can change them all to "nonw"
for(i in 1:length(title)){
  if (is.integer(train_org[,title[i]])){
    train_org[,title[i]][which(is.na(train_org[,title[i]]))] <- as.integer(0)
  } else {levels(train_org[,title[i]]) <- c(levels(train_org[,title[i]]), "None")
  train_org[,title[i]][is.na(train_org[,title[i]])] <- "None"
  }
}

# Masonry veneer, the same problem with Garage
title <- c("MasVnrArea", "MasVnrType")
result <- c()
for (i in 1:(length(title)-1)) {
  if (identical(as.character(train_org[is.na(train_org[,title[i]]) ,title[i]]), as.character(train_org[is.na(train_org[,title[i+1]]),
                                                                                                    title[i+1]]))) {
    result[i] <- paste(title[i], "equal", title[i+1], sep = " ")
  }else {
    result[i] <- paste(title[i], "not equal", title[i+1], sep = " ")
  }
}
print(result)
for(i in 1:length(title)){
  if (is.integer(train_org[,title[i]])){
    train_org[,title[i]][which(is.na(train_org[,title[i]]))] <- as.integer(0)
  } else {levels(train_org[,title[i]]) <- c(levels(train_org[,title[i]]), "None")
  train_org[,title[i]][is.na(train_org[,title[i]])] <- "None"
  }
}

# Basement, the same problem with Garage but the ID may different in different variable. For example, in the same entry, "BsmtFinType1" 
# may shows unfinished, but in "BsmtCond" may shows NA which means no basement.
title <- c("BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2")
for(i in 1:length(title)){
  if (is.integer(train_org[,title[i]])){
    train_org[,title[i]][which(is.na(train_org[,title[i]]))] <- as.integer(0)
  } else {levels(train_org[,title[i]]) <- c(levels(train_org[,title[i]]), "None")
  train_org[,title[i]][is.na(train_org[,title[i]])] <- "None"
  }
}

# LotFrontage and Electrical, since NAs in LotFrontage means unknown, I will KNN to impute the NAs

library(DMwR)
train_org <- knnImputation(train_org)

# check again to make sure there is no missing value
Missing_indices <- sapply(train_org, function(x)sum(is.na(x)))
Missing_Summary <- data.frame(index = names(train_org), Missing_Values = Missing_indices)
Missing_Summary[Missing_Summary$Missing_Values > 0,]

# we should remove the other identifier variables since they do not show any features for the obs.
# Otherwise, Month sold should be a factor since different month may different trend for trading house.
train_org$MoSold <- as.factor(train_org$MoSold)
train_control <- train_org[, -1]

# Now we need to see the distribution of SalePrice
ggplot(train_control,aes(SalePrice))+geom_histogram()

# Since SalePrice is skewed, taking the log of the variable normalizes it.
ggplot(train_control, aes(x=log(SalePrice+1))) + geom_histogram(col = 'white') + theme_light()

# Since PCA works on numeric variables, we'll convert these categroical variables into
# numeric variables. So we use dummy variables to transfer the categroical variables in to numeric variables.
library(dummies)
names <- names(train_control[sapply(train_control, is.factor)])
train_dummies <- dummy.data.frame(train_control, names = names)
str(train_dummies)

# now we can separate train data into train and test.
# create a sample vector of test values
testid <- sample(1:nrow(train_dummies), nrow(train_dummies)/4, replace = F)

# test dataset
test <- train_dummies[testid,]

# train dataset
train <- train_dummies[-testid,]

# After generating the dataset of train and test, we need to check the data frame again to make sure the data is correct.
str(train)
str(test)

# Since we don't want the variables have no changes, so we remove the variables with 0 variance
names <-  sapply(train, function(v) var(v)==0)
train <- train[,!names]
test <- test[, !names]

# Find out the index of "SalePrice"
index_SaleP <- which(colnames(train) == "SalePrice")

# PCA
# we normalize the variables to have standard deviation equals to
# 1 since it will be good for graph.
train_PCA <- prcomp(train[,-index_SaleP], scale. = T)
names(train_PCA)
var <- (train_PCA$sdev)**2
prop_varex <- var/sum(var)
plot(prop_varex, xlab = "Principal Component",
     ylab = "Proportion of Variable Explained",
     type = "b")
plot(cumsum(prop_varex), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     type = 'b')
which(cumsum(prop_varex) == 1)

# now we need to create new train data with PCA.
train.data <- data.frame(SalePrice = train$SalePrice, train_PCA$x)

# since we find out that the first 251 PCAs can explain 100%, so we just
# use the first 251 PCAs with SalePrice variable
train.data <- train.data[, 1:252]

# we also need to transform test into PCAs
test.data <- predict(train_PCA, newdata = test[, -index_SaleP])
test.data <- as.data.frame(test.data)
test.data <- test.data[, 1:251] # without SalePrice

# now we are going to do forecasting. There are two methods I can use. One is random forest, another is GBM. I will do the both and
# compare the two RMSE and find out which one is the best.

# Random Forest
model <- randomForest(SalePrice ~., data = train.data, method = "anova",
                      ntree = 1000,
                      replace = F,
                      importance = T)
predict <- predict(model, test.data)
RMSE1 <- RMSE(predict, test$SalePrice)
RMSE1 <- round(RMSE1, digits = 3)
plot1 <- predict-test$SalePrice

# GBM
model <- gbm(SalePrice ~., data = train.data, distribution = "laplace",
             shrinkage = 0.1,
             interaction.depth = 3,
             bag.fraction = 0.5,
             n.minobsinnode = 10,
             cv.folds = 10,
             keep.data = F,
             verbose = F,
             n.trees = 1000)
predict <- predict(model, test.data, n.trees = 1000)
RMSE2 <- RMSE(predict, test$SalePrice)
RMSE2 <- round(RMSE2, digits = 3)
plot2 <- predict-test$SalePrice

# Compare the two RMSE and choose the smallest one.
RMSE1 > RMSE2
